{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPtPAavAxrM0ojUIf7wsFRe"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"P8UCSz-6Tt12","executionInfo":{"status":"ok","timestamp":1742027702406,"user_tz":-330,"elapsed":95,"user":{"displayName":"N K","userId":"04064883306889132122"}}},"outputs":[],"source":[]},{"cell_type":"markdown","source":["optimization is about finding the best possible set of parameters that minimizes or maximizes a function. in the context of deep learning the goal is to minimize a loss function that measures how far off is the model's predictions in comparision with the actual or true values. it could be considered as a process by which a model adjusts parameters to learn. a well optimized model will have better performance and lower error. an optimized algorithm also makes training feasible for large or complex models.\n","\n","key components:\n","    - objective or loss function: the function that needs to be minimized\n","    - parameters: variables (weights or bias) that are adjusted to minimize the loss\n","    - gradient: gradient or derivative of the loss function with respect to the given parameters, which can be used to step in the direction of the steepest slope\n","    - rule for gradient: g_new = g_old - n dL(g_old) where g_ is the gradient\n","    -"],"metadata":{"id":"YJkkETE9Tved"}},{"cell_type":"code","source":["# example implementation of optimization with function f(x, y) = (x-2)^2 + (y+3)^2, the function reaches it maximum at (2, -3)\n","\n","def loss_function(params):\n","    # computes the value of the loss function\n","    x, y = params\n","    return (x-2)**2 + (y+3)**2\n","\n","# gradient of the loss function\n","def gradient(params):\n","    x, y = params\n","    grad_x, grad_y = 2(x-2), 2(y+2)\n","    return np.array([grad_x, grad_y])\n","\n","# gradient descent\n","def gradient_descent(initial_params, learning_rate, iterations):\n","\n","    params = initial_params.copy()\n","    param_history = [params.copy()]\n","    loss_history = [loss_function(params)]\n","\n","    for i in range(iterations):\n","        grad = gradient(params)\n","        params -= learning_rate * grad\n","\n","        params.history.append(params.copy())\n","        loss_history.append(loss_function(params))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IF82kr30bKem","executionInfo":{"status":"ok","timestamp":1742027702438,"user_tz":-330,"elapsed":15,"user":{"displayName":"N K","userId":"04064883306889132122"}},"outputId":"7ad8fec3-dea8-4e63-f030-9e9e7a898c59"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stderr","text":["<>:11: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n","<>:11: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n","<>:11: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n","<>:11: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n","<ipython-input-1-868be66d2e6c>:11: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n","  grad_x, grad_y = 2(x-2), 2(y+2)\n","<ipython-input-1-868be66d2e6c>:11: SyntaxWarning: 'int' object is not callable; perhaps you missed a comma?\n","  grad_x, grad_y = 2(x-2), 2(y+2)\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"ix2icEV08gbM"}},{"cell_type":"markdown","source":["perceptron: one of the essential concepts of deep learning that serves as a fundamental  building block and could be regarded as a simplest neuron or a unit in neural networks. it takes input values, applies weights to it, adds bias, and then uses an activation function, which then gives an output\n","- inputs: set of features that describe data point/s typically represented as a vector or a matrix, where each row corresponds to one sample or data, X = [x1, x2,...xn], where each xi can either be a scalar value or a row in a matrix\n","- weights: each input is multiplied by corresponding weights that reflect its importance in predictions or decision making process, w = [w1, w2, ...., wn], where the number of weights is equal to the number of columns in a input matrix or vector.\n","- bias: an extra term or a shift that pushes the decision boundary, it also has it's own corresponding weight\n","- weighted sum or summation: perceptron calculates weighted sum of the inputs and bias, z = (X . w)+ b; where if X is a matrix then it could be considered as a matrix vector multiplication and if it is a scalar it is then a dot product\n","- activation function: a function that converts weighted sum into a binary output, where if z or the input value for activation function is a scalar it gives a binary value of 0 or 1 and if it is a vector it performs a element-wise and produces a vector of binary values\n","activation_func(z) = {1, if z >= n ;   0, otherwise where n could be considered as a threshold number }"],"metadata":{"id":"I7wyNTNBe_6a"}},{"cell_type":"markdown","source":["learning/training process:\n","- prediction: for each input, perceptron calculates the output\n","- error calculation: the output is compared with the true labels or the expected outputs (where expected outputs are the actual values that the model is trying to predict)\n","- weight update: weights and bias are adjusted to reduce the calculated error, where the new respective weights would reduce the error by trying to minimize the distance"],"metadata":{"id":"D-E3ZPKUAR4_"}},{"cell_type":"markdown","source":["torch.nn: a comprehensive library that supports construction of neural networks with pre-defined classess and functons that streamline the process of building, training, and deploying deep learning models.\n","- supports variety of layer implementations\n","    - linear layers: nn.Linear\n","    - convolutional layers: nn.Con2d, nn.Con3d for convolutional operations\n","    - recurrent layers: nn.RNN, nn.LSTM, nn.GRU for sequential data processing\n","    - normalization layers: nn.BatchNorm2d, nn.LayerNorm for normalizing activations"],"metadata":{"id":"hCayeAK9Fp1E"}},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","\n","# simple perceptron module using torch\n","class Perceptron(nn.Module):\n","    def __init__(self, input_dim):\n","        \"\"\"\n","        initializes perceptron\n","        args:\n","            - input_dim: number of features in the input data\n","        \"\"\"\n","        # initializing by calling the super method to access nn.Module\n","        super(Perceptron, self).__init__()\n","\n","        # performs input * weight + bias\n","        self.linear = nn.Linear(in_features=input_dim, out_features=1) #in_features: dimension of the input, out_features: dimension of the output\n","\n","    def forward(self, x):\n","        \"\"\"\n","        performs the foward pass of the perceptron\n","        args:\n","            - x (torch.Tensor): input tensor of the shape (batch_size, input_dim)\n","\n","        returns:\n","            - torch.Tensor: output tensor after applying the activation function\n","        \"\"\"\n","        z = self.linear(x)\n","\n","        # returns a tensor of 0s for x < 0 and 1s for x >= 0\n","        out = torch.heaviside(z, torch.tensor([0.0]))\n","        return out\n"],"metadata":{"id":"OXoqZSvZgdQZ","executionInfo":{"status":"ok","timestamp":1742033997465,"user_tz":-330,"elapsed":10,"user":{"displayName":"N K","userId":"04064883306889132122"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rt5J2d4vl4Ij"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","class SimpleNN(nn.Module):\n","    def __init__(self, input_dim, hidden_dim, output_dim):\n","        \"\"\"\n","        intializes the network layers\n","        args:\n","            - input_dim: number of input dimensions\n","            - hidden_dim: number of neurons in the hidden layer\n","            - output_dim: number of output dimensions\n","        \"\"\"\n","        super(SimpleNN, self).__init__()\n","\n","        # a linear layer for input -> hidden\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","\n","        # a linear layer for hidden -> output\n","        self.fc2 = nn.Linear(hidden_dim, output_dim)\n","\n","\n","    def forward(self, x):\n","        \"\"\"\n","        forward pass through the network\n","        args:\n","            - x (torch.Tensor): input tensor of shpae (batch_size, input_dim)\n","        returns (torch.Tensor): output tensor of shpae (batch_size, input_dim)\n","        \"\"\"\n","        hidden = F.relu(self.fc1(x))\n","        output = self.fc2(hidden)\n","\n","        return output"],"metadata":{"id":"mfooeZcycNgq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"EbF6FJWD9LJt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","training loop for perceptron: implements perceptron learning algorithm\n","    - forward propagation: the input passes through network to get a prediction\n","    - compares the prediction with true labels\n","    - if the prediction is incorrect, weights and bias are updated\n","\"\"\"\n","\n","def train_perceptron(model, inputs, target, learning_rate=0.01, epochs=10):\n","    \"\"\"\n","    train the perceptron model using the perceptrons learning rule\n","    args:\n","        - model (perceptron): an instance or object of the perceptron\n","        - inputs (torch.Tensor): input data of shape (num_samples, input_dim)\n","        - targets (torch.Tensor): binary labels of samples (num_samples, 1)\n","        - learning_rate(float): learning rate for updating weights and bias\n","        - epochs(int): number of times to iterate over the entire dataset\n","\n","    returns:\n","        int: number of epochs taken until it converges\n","    \"\"\"\n","    num_samples = inputs.size(0)\n","\n",""],"metadata":{"id":"3yZJ9g_WUdTN"},"execution_count":null,"outputs":[]}]}